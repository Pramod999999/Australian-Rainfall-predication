# -*- coding: utf-8 -*-
"""Group_9

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JgsnNJatfOB26Ffml2y1qUnON5ZMzsge
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

class LogisticRegression:
    def __init__(self, learning_rate, no_of_iterations):

        self.learning_rate = learning_rate

        self.no_of_iterations = no_of_iterations



    def fit(self, x, y):
        self.m, self.n = x.shape

        self.w = np.zeros(self.n)

        self.b = 0

        self.x = x

        self.y = y

        for i in range(self.no_of_iterations):

            self.update_weights()

    def update_weights(self):

        y_predicted = 1 / (1 + np.exp(-(self.x.dot(self.w)+ self.b)))

        dw = (1 / self.m) * np.dot(self.x.T, (y_predicted - self.y))

        db = (1 / self.m) * np.sum(y_predicted - self.y)

        self.w = self.w - self.learning_rate * dw

        self.b = self.b - self.learning_rate * db


    def predict(self, x):

        y_prediction = 1 / (1 + np.exp(-(x.dot(self.w) + self.b)))

        y_prediction = np.where(y_prediction > 0.5, 1, 0)
        return y_prediction

data = pd.read_csv("/content/weatherAUS.csv")
data

data.info()

data.head()

data.isnull().sum()

#Since out of 82000 columns 62790 are missing in Evaporation and in sunshine out 75000 entries 69800 are empty . So dropping them out
data = data.drop(["Evaporation","Sunshine"],axis=1)

data

data["WindDir9am"].dtypes

numerical_feature = [feature for feature in data.columns if data[feature].dtypes != "O"]
discrete_feature=[feature for feature in numerical_feature if len(data[feature].unique())<25]
continuous_feature = [feature for feature in numerical_feature if feature not in discrete_feature]
categorical_feature = [feature for feature in data.columns if feature not in numerical_feature]
print("Numerical Features Count {}".format(len(numerical_feature)))
print("Discrete feature Count {}".format(len(discrete_feature)))
print("Continuous feature Count {}".format(len(continuous_feature)))
print("Categorical feature count {}".format(len(categorical_feature)))

numerical_feature

discrete_feature

categorical_feature

data.head(20)

data.isnull().sum()

def randomsamplelimputation(data,variable):
  data[variable]=data[variable]
  random_sample=data[variable].dropna().sample(data[variable].isnull().sum(),random_state=0)
  random_sample.index=data[data[variable].isnull()].index
  data.loc[data[variable].isnull(),variable]=random_sample

randomsamplelimputation(data,"Cloud3pm")
randomsamplelimputation(data,'WindDir9am')
randomsamplelimputation(data,'RainToday')
randomsamplelimputation(data,'MinTemp')
randomsamplelimputation(data,'MaxTemp')
randomsamplelimputation(data,'Rainfall')
randomsamplelimputation(data,'WindGustDir')
randomsamplelimputation(data,'WindDir3pm')
randomsamplelimputation(data,'WindSpeed9am')
randomsamplelimputation(data,'WindSpeed3pm')
randomsamplelimputation(data,'WindGustSpeed')
randomsamplelimputation(data,'Humidity9am')
randomsamplelimputation(data,'Humidity3pm')
randomsamplelimputation(data,'Pressure9am')
randomsamplelimputation(data,'Pressure3pm')
randomsamplelimputation(data,'Cloud9am')
randomsamplelimputation(data,'Temp9am')
randomsamplelimputation(data,'Temp3pm')

data.isnull().sum()

data.dropna(inplace=True)#
data.isnull().sum()

fig, axes = plt.subplots(11, 2, figsize=(20,60 ))
axes = axes.ravel()
for i, column in enumerate(data.columns):
  ax = axes[i]
  ax.hist(data[column], bins=30, color='blue', alpha=0.7)
  ax.set_title(column)
  ax.set_xlabel('Value')
  ax.set_ylabel('Frequency')

data

corr=data.corr()

corr.style.background_gradient(cmap = 'coolwarm')



dummies = pd.get_dummies(data[['Date', 'Location', 'WindDir9am', 'WindDir3pm', 'RainToday', 'RainTomorrow',"WindGustDir"]])

data[['Date', 'Location', 'WindDir9am', 'WindDir3pm', 'RainToday', 'RainTomorrow']]

dummies

# Concatenating the original dataset with the dummy variables
data = pd.concat([data, dummies], axis=1)
# Dropping the original categorical columns
data = data.drop(['Date', 'Location', 'WindDir9am', 'WindDir3pm', 'RainToday', 'RainTomorrow',"WindGustDir"], axis=1)

new_data = data.head( 2000)

new_data

categorical_feature

X = new_data.drop(["RainTomorrow_No","RainTomorrow_Yes"],axis= 1)
Y = new_data["RainTomorrow_Yes"]


len(X)

new_data.info()

X.columns

# Splittinh the data into training and testing sets

X_train = X[:1600]
Y_train = Y[:1600]
X_test= X[1600:2000]
Y_test = Y[1600:2000]


X[1:2]

model = LogisticRegression(0.01,100)

numerical_feature

model.fit(X_train,Y_train)

X_test

Y_test.shape

X_test.shape

predictions=model.predict(X_test)

predictions

accuracy = np.sum(predictions == Y_test) / len(Y_test)
accuracy

model_2 = LogisticRegression(0.001,10000)

model_2.fit(X_train,Y_train)

X_test

Y_test.shape

X_train.shape
Y_train.shape

predictions=model_2.predict(X_test)

predictions

accuracy = np.sum(predictions == Y_test) / len(Y_test)
accuracy

